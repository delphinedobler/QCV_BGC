{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "52f6cf8a-d797-4743-85e9-4a3c788aa469",
   "metadata": {},
   "source": [
    "# COLOCATION TOOL FOR IN-SITU and COPERNICUS GRIDDED PRODUCTS "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f065253-fa25-4552-9685-617835846bce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ADAPTED TO ARGO/GLIDER NEEDS FOR CHL VALIDATION\n",
    "# Author: D.Dobler (Euro-Argo ERIC)\n",
    "# Date: 2024-12-04\n",
    "\n",
    "\n",
    "# comments:\n",
    "\n",
    "# Use of the copernicus marine service library\n",
    "\n",
    "# First the subset method was tested\n",
    "# Second the opendataset method is tested (was is referred to as \"lazy load\", even if I don't really like this term that does not tell what's behind: this is an index loading)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d19f2f34",
   "metadata": {},
   "source": [
    "<!--TABLE OF CONTENTS-->\n",
    "Contents:\n",
    "- [COLOCATION TOOL FOR IN-SITU and COPERNICUS GRIDDED PRODUCTS](#COLOCATION-TOOL-FOR-IN-SITU-and-COPERNICUS-GRIDDED-PRODUCTS)\n",
    "  - [I - Libraries imports and credentials handling](#I---Libraries-imports-and-credentials-handling)\n",
    "  - [II - Main functions](#II---Main-functions)\n",
    "    - [II.a - copernicus_marine subset function](#II.a---copernicus_marine-subset-function)\n",
    "    - [II.b - Argo data related functions - direct access](#II.b---Argo-data-related-functions---direct-access)\n",
    "    - [II.c Cerbere files related functions](#II.c-Cerbere-files-related-functions)\n",
    "  - [II.d - get all observations for one workflow](#II.d---get-all-observations-for-one-workflow)\n",
    "    - [II.e - Distance computation function](#II.e---Distance-computation-function)\n",
    "    - [II.f - In-situ observation grouping function](#II.f---In-situ-observation-grouping-function)\n",
    "  - [III - Colocation](#III---Colocation)\n",
    "    - [III.a - configuration selection](#III.a---configuration-selection)\n",
    "    - [III.b - IN-SITU data selection](#III.b---IN-SITU-data-selection)\n",
    "    - [III.b - Define needed datasets and variables for Chlorophyll-A](#III.b---Define-needed-datasets-and-variables-for-Chlorophyll-A)\n",
    "    - [III.c - spatial resolution and boundaries of the copernicus datasets](#III.c---spatial-resolution-and-boundaries-of-the-copernicus-datasets)\n",
    "    - [III.d - group extraction by geograpical criterion](#III.d---group-extraction-by-geograpical-criterion)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb834c76-c1a5-466f-8f0c-47a845db028a",
   "metadata": {},
   "source": [
    "## I - Libraries imports and credentials handling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7258d407-a5aa-4888-b8a7-ee73121ce524",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copernicusmarine\n",
    "# Nota Bene: Copernicusmarine (both python and CLI) does not work when Ivanti is active. \n",
    "import requests\n",
    "import xarray as xr # xarray methods can sometimes be quite long, not sure why. Changed for netCDF4 library\n",
    "from netCDF4 import Dataset\n",
    "import netCDF4 as nc\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime as dt\n",
    "import time\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import traceback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61f73c30-342c-46f5-90aa-dfc7a05e6f66",
   "metadata": {},
   "outputs": [],
   "source": [
    "str(np.datetime64('today','D'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8659f1fb-c734-406a-8b4c-48781ce273f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To know all the options from the service, uncomment the following line:\n",
    "#?copernicusmarine\n",
    "#?copernicusmarine.subset\n",
    "?copernicusmarine.open_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0c8a9a2-addf-4525-8c9d-10bd46bf836c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Credentials Handling\n",
    "# For credentials, it is possible to use the following environment variables, but I was not able to get them working.\n",
    "# on unix:\n",
    "# export COPERNICUSMARINE_SERVICE_USERNAME=your_username\n",
    "# export COPERNICUSMARINE_SERVICE_PASSWORD=your_password\n",
    "# on windows:\n",
    "# set COPERNICUSMARINE_SERVICE_USERNAME=your_username\n",
    "# set COPERNICUSMARINE_SERVICE_PASSWORD=your_password\n",
    "# within the notebook:\n",
    "# %env COPERNICUSMARINE_SERVICE_USERNAME=your_username\n",
    "# %env COPERNICUSMARINE_SERVICE_PASSWORD=your_password\n",
    "\n",
    "# Finally, I found where it was indicated in the following FAQ\n",
    "# (https://help.marine.copernicus.eu/en/articles/8684964-i-m-an-operational-user-what-should-i-know-to-use-the-copernicus-marine-toolbox)\n",
    "# copernicusmarine.login()\n",
    "# it saved my credential within:\n",
    "# C:\\Users\\ddobler\\.copernicusmarine\\.copernicusmarine-credentials"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "128a5867-ee83-48e8-a46f-7bf35e7bf6fc",
   "metadata": {},
   "source": [
    "## II - Main functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dea561b6-7c25-4263-a5d5-a3d0ce2264b9",
   "metadata": {},
   "source": [
    "### II.a - copernicus_marine subset function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bd4b007-7d22-4f0f-ae5e-a0f5ed16c071",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cms_data(did,var,lonm,lonp,latm,latp,datm,datp,zm,zp,outd,outf):\n",
    "    copernicusmarine.subset(\n",
    "      dataset_id=did,\n",
    "      variables=var,\n",
    "      minimum_longitude=lonm,\n",
    "      maximum_longitude=lonp,\n",
    "      minimum_latitude=latm,\n",
    "      maximum_latitude=latp,\n",
    "      start_datetime=datm,\n",
    "      end_datetime=datp,\n",
    "      minimum_depth=zm,\n",
    "      maximum_depth=zp,\n",
    "      output_filename = outf,\n",
    "      output_directory = outd,\n",
    "      force_download=True, # Important, otherwise a prompt asks for downloading confirmation.\n",
    "      overwrite_output_data=True # important because if False (default value): when the output file already exists, it adds a (n) at the end. This can prevent from fetching the correct file name\n",
    "      \n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38719c29-c878-44e2-808e-a719a81f876f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_workflow_dataset_and_var(workflow_name):\n",
    "\n",
    "    if workflow_name == \"chl\":\n",
    "        \n",
    "        dataset_rrs='cmems_obs-oc_glo_bgc-reflectance_my_l3-multi-4km_P1D'\n",
    "        rrs_var=['RRS412','RRS443','RRS490','RRS555','RRS670']\n",
    "        dataset_chl='cmems_obs-oc_glo_bgc-plankton_my_l3-multi-4km_P1D'\n",
    "        chl_var=['CHL']\n",
    "        dataset_Kd='cmems_obs-oc_glo_bgc-transp_my_l3-multi-4km_P1D'\n",
    "        Kd_var=['KD490']\n",
    "\n",
    "        l_dataset=[dataset_chl,dataset_rrs,dataset_Kd]\n",
    "        \n",
    "        d_dataset_var={}\n",
    "        d_dataset_var[dataset_chl]=chl_var\n",
    "        d_dataset_var[dataset_rrs]=rrs_var\n",
    "        d_dataset_var[dataset_Kd]=Kd_var\n",
    "\n",
    "    return l_dataset,d_dataset_var\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78360043-fdcd-4b43-b3d7-4182fabd3f62",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_resolution(workflow_name,method,clear_cache=False,cache_dir='cache_files',verbose=False):\n",
    "\n",
    "    # intialisation (list of the datasets spatio-temporal features or stf)\n",
    "    i_dataset_stf={}\n",
    "    l_dataset_stf={}\n",
    "\n",
    "    # workflow datasets and vars:\n",
    "    l_dataset,d_dataset_var=get_workflow_dataset_and_var(workflow_name)\n",
    "\n",
    "    # Test is a cache file with value is present\n",
    "    if not os.path.exists(cache_dir):\n",
    "        os.mkdir(cache_dir)\n",
    "    cache_resolution_file = cache_dir + \"/cache_datasets_for_\"+workflow_name+\"_workflow_spatial_resolution.txt\"\n",
    "\n",
    "    if (os.path.exists(cache_resolution_file)) & (clear_cache):\n",
    "        os.remove(cache_resolution_file)\n",
    "        if verbose:print(\"the cache file was cleared\")\n",
    "    \n",
    "    if not os.path.exists(cache_resolution_file):\n",
    "\n",
    "        # Initialise the cache file\n",
    "        file = open(cache_resolution_file, 'w')\n",
    "        line2write = \"dataset_id;reso_lon_deg;reso_lat_deg;lon_min;lon_max;lat_min;lat_max;reso_time_ns;time_min;time_max\"\n",
    "        file.write(line2write + '\\n')\n",
    "        file.close()\n",
    "        \n",
    "        for idataset in l_dataset:\n",
    "            try:\n",
    "                #if method=='lazy':\n",
    "                if verbose:print(\"\\n\\n\\n ******* Reading spatio-temporal features for \" + idataset)\n",
    "                ds=copernicusmarine.open_dataset(dataset_id=idataset)#,dataset_part='default',service='arco-geo-series')\n",
    "                #print(ds)\n",
    "                #if method=='subset':\n",
    "                #    get_cms_data(idataset,d_dataset_var[idataset],\n",
    "                #             0,0,0,0,\"2022-06-01T00:00:00\",\"2022-06-01T00:00:00\",0,0,\"copernicus-data\",\n",
    "                #             idataset+\"_reso.nc\")\n",
    "                #    ds=xr.open_dataset(\"copernicus-data/\"+idataset+\"_reso.nc\")\n",
    "                i_dataset_stf['reso_lon_deg']=ds.attrs['lon_step']\n",
    "                i_dataset_stf['reso_lat_deg']=ds.attrs['lat_step']\n",
    "                i_dataset_stf['spat_lon_min']=ds.attrs['geospatial_lon_min']\n",
    "                i_dataset_stf['spat_lon_max']=ds.attrs['geospatial_lon_max']\n",
    "                i_dataset_stf['spat_lat_min']=ds.attrs['geospatial_lat_min']\n",
    "                i_dataset_stf['spat_lat_max']=ds.attrs['geospatial_lat_max']\n",
    "                # The global acceptable min and max in time assumes to use open_dataset (\"lazy\" method). There\n",
    "                # is no global attribute helping in retrieving this boundary when subsetting.\n",
    "                # The global attributes for time min and max (time_coverage_start and time_coverage_end) \n",
    "                # are wrongly filled: we must compute min and max from the time coordinate.\n",
    "                # the time_coverage_resolution global attribute is a string, tricky to parse; here again, it is\n",
    "                # better to recompute from the time coordinate\n",
    "                #i_dataset_stf['reso_time']=ds.attrs['time_coverage_resolution']\n",
    "                #i_dataset_stf['time_min']=ds.attrs['time_coverage_start']\n",
    "                #i_dataset_stf['time_max']=ds.attrs['time_coverage_end']\n",
    "                i_dataset_stf['time_min']=np.array(ds['time'].min())\n",
    "                i_dataset_stf['time_max']=np.array(ds['time'].max())\n",
    "                i_dataset_stf['reso_time_ns']=np.timedelta64(np.mean(np.array(ds['time'][1:])-np.array(ds['time'][:-1])),'ns')\n",
    "            \n",
    "                l_dataset_stf[idataset]=i_dataset_stf\n",
    "                ds.close()\n",
    "            except:\n",
    "                print(\"ERROR: while downloading data from cmems with the \" + method + \" method\")\n",
    "                print(traceback.format_exc())\n",
    "\n",
    "            try:\n",
    "                file = open(cache_resolution_file, 'a')\n",
    "                reso_time_str=str(np.timedelta64(i_dataset_stf['reso_time_ns'],'ns')/np.timedelta64(1,'ns'))\n",
    "                line2write = idataset + \";\" + str(i_dataset_stf['reso_lon_deg']) + \";\" + str(i_dataset_stf['reso_lat_deg']) + \";\" + \\\n",
    "                             str(i_dataset_stf['spat_lon_min']) + \";\" + str(i_dataset_stf['spat_lon_max']) + \";\" + \\\n",
    "                             str(i_dataset_stf['spat_lat_min']) + \";\" + str(i_dataset_stf['spat_lat_max']) + \";\" + \\\n",
    "                             reso_time_str + \";\" + str(i_dataset_stf['time_min']) + \";\" + str(i_dataset_stf['time_max'])\n",
    "                file.write(line2write + '\\n')\n",
    "                \n",
    "                file.close()\n",
    "            except:\n",
    "                print(\"ERROR: while writing the cache_resolution file: \" + cache_resolution_file)\n",
    "                file.close()\n",
    "                if os.path.exists(cache_resolution_file):\n",
    "                    os.remove(cache_resolution_file)\n",
    "                print(traceback.format_exc())\n",
    "    else:\n",
    "        if verbose: print(\"Reading spatial resolution and boundaries from cache file\")\n",
    "        Reso_index=pd.read_csv(cache_resolution_file,sep=\";\")\n",
    "        for idataset in l_dataset:\n",
    "            i_dataset_stf['reso_lon_deg']=Reso_index['reso_lon_deg'][np.where(Reso_index['dataset_id']==idataset)[0][0]]\n",
    "            i_dataset_stf['reso_lat_deg']=Reso_index['reso_lat_deg'][np.where(Reso_index['dataset_id']==idataset)[0][0]]\n",
    "            i_dataset_stf['spat_lon_min']=Reso_index['lon_min'][np.where(Reso_index['dataset_id']==idataset)[0][0]]\n",
    "            i_dataset_stf['spat_lon_max']=Reso_index['lon_max'][np.where(Reso_index['dataset_id']==idataset)[0][0]]\n",
    "            i_dataset_stf['spat_lat_min']=Reso_index['lat_min'][np.where(Reso_index['dataset_id']==idataset)[0][0]]\n",
    "            i_dataset_stf['spat_lat_max']=Reso_index['lat_max'][np.where(Reso_index['dataset_id']==idataset)[0][0]]\n",
    "            reso_time_ns_int64=(Reso_index['reso_time_ns'][np.where(Reso_index['dataset_id']==idataset)[0][0]]).astype('int64')\n",
    "            i_dataset_stf['reso_time_ns']=np.timedelta64(reso_time_ns_int64,'ns')\n",
    "            i_dataset_stf['time_min']=np.datetime64(Reso_index['time_min'][np.where(Reso_index['dataset_id']==idataset)[0][0]])\n",
    "            i_dataset_stf['time_max']=np.datetime64(Reso_index['time_max'][np.where(Reso_index['dataset_id']==idataset)[0][0]])\n",
    "            l_dataset_stf[idataset]=i_dataset_stf\n",
    "\n",
    "    if verbose:\n",
    "        for idataset in l_dataset:\n",
    "            print(\"\\n For copernicus dataset: \" + idataset)\n",
    "            print(\"reso_longitude = \",i_dataset_stf['reso_lon_deg'],\" deg, reso_latitude = \",i_dataset_stf['reso_lat_deg'],\"deg\")\n",
    "            print(\"spat_lon_min   = \",i_dataset_stf['spat_lon_min'],\" deg, spat_lon_max  = \",i_dataset_stf['spat_lon_max'],\"deg\")\n",
    "            print(\"spat_lat_min   = \",i_dataset_stf['spat_lat_min'],\" deg, spat_lat_max  = \",i_dataset_stf['spat_lat_max'],\"deg\")\n",
    "            print(\"reso_time_ns = \",i_dataset_stf['reso_time_ns'])\n",
    "            print(\"time_min   = \",i_dataset_stf['time_min'],\" , time_max  = \",i_dataset_stf['time_max'])\n",
    "            print(i_dataset_stf['reso_time_ns'].dtype)\n",
    "            print(i_dataset_stf['time_min'].dtype)\n",
    "\n",
    "\n",
    "    return l_dataset_stf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16a08cd3-6215-41e5-96f1-d10c98e67b0d",
   "metadata": {},
   "source": [
    "### II.b - Argo data related functions - direct access"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84df192a-eaae-4df5-bde8-2850038e1c89",
   "metadata": {},
   "outputs": [],
   "source": [
    "def qc_from_char2int(val):\n",
    "    shape_ini=val.shape\n",
    "    tmp=((np.array(val)).astype('|S1')).tobytes().decode()\n",
    "    tmp=tmp.replace(' ','6') # beware: this is only for computational issue, QC6 is unused usually\n",
    "    tmp=list(tmp)\n",
    "    out_val=np.array(tmp,dtype='int')\n",
    "    out_val=np.reshape(out_val,shape_ini)\n",
    "    return out_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1cd51ca-6dfb-4e77-bf6e-d1584befac9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_file_from_url(URL,DL_FILE):\n",
    "    response = requests.get(URL)\n",
    "    if response.status_code == 404:\n",
    "        print(\"No \" + URL + \" found in the gdac\")\n",
    "    else:\n",
    "        open(DL_FILE, \"wb\").write(response.content)\n",
    "        print(URL + \" found in the gdac, locally downloaded in \" + DL_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83f0e0f7-3050-406f-9002-f33f84455112",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_and_read_Argo_meta_index():\n",
    "    META_index_file=\"ar_index_global_meta.csv\"\n",
    "    URL = \"https://data-argo.ifremer.fr/ar_index_global_meta.txt\"\n",
    "    get_file_from_url(URL,META_index_file)\n",
    "    META_index=pd.read_csv(META_index_file,header=9,sep=\"/\",names=['dac','wmo','remaining'],dtype={'dac': 'str', 'wmo': 'str', 'remaining': 'str'})\n",
    "    return META_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eb04ca1-8e96-4006-b7cc-4b9ed5011cdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dac_from_meta_index(wmo):\n",
    "    META_index=get_and_read_Argo_meta_index()\n",
    "    dac=META_index['dac'][np.where(META_index['wmo']==wmo)[0][0]]\n",
    "    return dac"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5288fc61-f8c5-4328-89c3-9b3af463ae67",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_argo_data_from_direct_access(wmo,workflow_name):\n",
    "    SPROF_FILE=wmo + \"_Sprof.nc\"\n",
    "    dac = get_dac_from_meta_index(wmo)\n",
    "    URL = \"https://data-argo.ifremer.fr/dac/\"+dac+\"/\"+wmo+\"/\"+wmo+\"_Sprof.nc\"\n",
    "    get_file_from_url(URL,SPROF_FILE)\n",
    "\n",
    "    # xarray was sometimes taking several seconds for an unknown reason\n",
    "    # As there is no challenge here in terms loading capacity\n",
    "    # the NetCDF.Dataset was used: it never showed the delay experienced with xarray, thus kept\n",
    "    ds=Dataset(SPROF_FILE,'r')\n",
    "    ds.set_auto_mask(False) # to avoid masked array, a little bit more tricky to manage\n",
    "    longitudes=ds.variables['LONGITUDE'][:]\n",
    "    latitudes=ds.variables['LATITUDE'][:]\n",
    "    position_qc=qc_from_char2int(ds.variables['POSITION_QC'][:])\n",
    "    JULD=ds.variables['JULD'][:]\n",
    "    dates_qc=qc_from_char2int(ds.variables['JULD_QC'][:])\n",
    "    cycles=ds.variables['CYCLE_NUMBER'][:]\n",
    "    PRES=ds.variables['PRES'][:]\n",
    "    if workflow_name == 'chl':\n",
    "        CHLA=ds.variables['CHLA'][:]\n",
    "        CHLA_QC=ds.variables['CHLA_QC'][:]\n",
    "        \n",
    "    #print(ds)\n",
    "    ds.close()\n",
    "    \n",
    "    ref_date=np.datetime64(\"1950-01-01T00:00:00\")\n",
    "    dates=JULD*86400*np.timedelta64(1, 's')+ref_date\n",
    "    \n",
    "    # output for colocation computation (1-D)\n",
    "    df=pd.DataFrame({'CYCLE':cycles,'DATE': dates, 'LAT': latitudes, 'LON': longitudes, 'DATE_QC': dates_qc, 'POSITION_QC': position_qc})\n",
    "\n",
    "\n",
    "    # output for colocation display\n",
    "    n_prof,n_levels=PRES.shape\n",
    "    prof=np.arange(n_prof)\n",
    "    levels=np.arange(n_levels)\n",
    "    \n",
    "    ds = xr.Dataset(\n",
    "        data_vars=dict(\n",
    "            DATE=([\"prof\"], dates),\n",
    "            LAT=([\"prof\"], latitudes),\n",
    "            LON=([\"prof\"], longitudes),\n",
    "            CYCLE=([\"prof\"], cycles),\n",
    "            DATE_QC=([\"prof\"], dates_qc),\n",
    "            POSITION_QC=([\"prof\"], position_qc),\n",
    "            PRES=([\"prof\", \"levels\"], PRES),\n",
    "            CHLA=([\"prof\", \"levels\"], CHLA),\n",
    "            CHLA_QC=([\"prof\", \"levels\"], CHLA_QC),            \n",
    "        ),\n",
    "        coords=dict(\n",
    "            prof=prof,\n",
    "            n_levels=levels,\n",
    "        ),\n",
    "        attrs=dict(description=\"Observation related data\"),\n",
    "    )\n",
    "\n",
    "    return df,ds\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "108bd2cf-7313-4e7d-9c8b-10a9a455e3c8",
   "metadata": {},
   "source": [
    "### II.c Cerbere files related functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9256883-c75e-4a8a-a05f-6cece670e693",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_argo_data_from_cerbere_access(cerbere_dir,wmo,workflow_name):\n",
    "    \n",
    "    cerbere_file = cerbere_dir + \"gdac_\" + wmo + \"_202212_harm_agg.nc\"\n",
    "    ds=Dataset(cerbere_file,'r')\n",
    "    ds.set_auto_mask(False) # to avoid masked array, a little bit more tricky to manage\n",
    "    longitudes=ds.variables['lon'][:]\n",
    "    latitudes=ds.variables['lat'][:]\n",
    "    position_qc=qc_from_char2int(ds.variables['pos_qc'][:])\n",
    "    JULD=ds.variables['time'][:]\n",
    "    dates_qc=qc_from_char2int(ds.variables['time_qc'][:])\n",
    "    cycles=ds.variables['cycle_number'][:]\n",
    "    PRES=ds.variables['pressure_raw'][:]\n",
    "    if workflow_name == 'chl':\n",
    "        CHLA=ds.variables['chlorophylle_raw'][:]\n",
    "        CHLA_QC=ds.variables['chlorophylle_raw_qc'][:]\n",
    "\n",
    "    print(ds)\n",
    "    ds.close()\n",
    "    \n",
    "    ref_date=np.datetime64(\"1950-01-01T00:00:00\")\n",
    "    dates=JULD*np.timedelta64(1, 's')+ref_date # In cerbere format, time units are seconds from ref-date\n",
    "    \n",
    "\n",
    "    # output for colocation computation (1-D)\n",
    "    df=pd.DataFrame({'CYCLE':cycles,'DATE': dates, 'LAT': latitudes, 'LON': longitudes, 'DATE_QC': dates_qc, 'POSITION_QC': position_qc,})\n",
    "\n",
    "    \n",
    "    # output for colocation display\n",
    "    n_prof,n_levels=PRES.shape\n",
    "    prof=np.arange(n_prof)\n",
    "    levels=np.arange(n_levels)\n",
    "    \n",
    "    ds = xr.Dataset(\n",
    "        data_vars=dict(\n",
    "            DATE=([\"prof\"], dates),\n",
    "            LAT=([\"prof\"], latitudes),\n",
    "            LON=([\"prof\"], longitudes),\n",
    "            CYCLE=([\"prof\"], cycles),\n",
    "            DATE_QC=([\"prof\"], dates_qc),\n",
    "            POSITION_QC=([\"prof\"], position_qc),\n",
    "            PRES=([\"prof\", \"levels\"], PRES),\n",
    "            CHLA=([\"prof\", \"levels\"], CHLA),\n",
    "            CHLA_QC=([\"prof\", \"levels\"], CHLA_QC),            \n",
    "        ),\n",
    "        coords=dict(\n",
    "            prof=prof,\n",
    "            n_levels=levels,\n",
    "        ),\n",
    "        attrs=dict(description=\"Observation related data\"),\n",
    "    )\n",
    "\n",
    "    return df,ds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02b41f7f-1d4d-4185-bc03-b2f3ebfc677e",
   "metadata": {},
   "source": [
    "## II.d - get all observations for one workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ca51f51-f3df-4b82-ae14-6418e358fc17",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_argo_data_from_index(workflow_name):\n",
    "    \n",
    "    BIO_Index_file=\"argo_bio-profile_index.csv\"\n",
    "    URL = \"https://data-argo.ifremer.fr/argo_bio-profile_index.txt\"\n",
    "    get_file_from_url(URL,BIO_Index_file)\n",
    "    BIO_Index=pd.read_csv(BIO_Index_file,header=8,sep=\",\")\n",
    "\n",
    "    # Removing lines with incomplete coordinates:\n",
    "    #print(\"intial size : \",BIO_Index.shape)\n",
    "    BIO_Index.drop(BIO_Index.index[BIO_Index['date'].isnull()],inplace=True)\n",
    "    #print(\"after removing null dates : \",BIO_Index.shape)\n",
    "    BIO_Index.drop(BIO_Index.index[BIO_Index['latitude'].isnull()],inplace=True)\n",
    "    #print(\"after removing null latitudes : \",BIO_Index.shape)\n",
    "    BIO_Index.drop(BIO_Index.index[BIO_Index['longitude'].isnull()],inplace=True)\n",
    "    #print(\"after removing null longitudes : \",BIO_Index.shape)\n",
    "\n",
    "    # Keeping lines including the worklow parameter:\n",
    "    BIO_Index=BIO_Index[BIO_Index['parameters'].str.contains(\"CHLA\")]\n",
    "    #print(\"after selecting chla lines : \",BIO_Index.shape)\n",
    "  \n",
    "   \n",
    "    latitudes=np.array(BIO_Index['latitude'])\n",
    "    longitudes=np.array(BIO_Index['longitude'])\n",
    "    dates=BIO_Index['date'].astype(str)\n",
    "    dates_str_iso_8601=dates.str[:4]+\"-\"+dates.str[4:6]+\"-\"+dates.str[6:8]+\"T\"+dates.str[8:10]+\":\"+dates.str[10:12]+\":\"+dates.str[12:14]\n",
    "    dates_dt64=np.array(dates_str_iso_8601,dtype='datetime64')\n",
    "    #print(dates_dt64)\n",
    "    \n",
    "    \n",
    "    dates_qc=np.ones(dates_dt64.shape)\n",
    "    position_qc=np.ones(dates_dt64.shape)\n",
    "    \n",
    "    df=pd.DataFrame({'DATE': dates_dt64, 'LAT': latitudes, 'LON': longitudes, 'DATE_QC': dates_qc, 'POSITION_QC': position_qc})\n",
    "\n",
    "    return df #,ds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76d4fdba-69a6-427c-bbe1-c3c4839202fb",
   "metadata": {},
   "source": [
    "### II.e - Distance computation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e4b289f-ce8a-4c59-b2bc-297497b58dbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "##########################################################################################\n",
    "##########################################################################################\n",
    "def compute_earth_radius_elliptical(lat_deg):\n",
    "    \n",
    "    # This function returns the earth radius at a given latitude, assuming an\n",
    "    # elliptical earth model.\n",
    "    \n",
    "    if type(lat_deg)==np.ndarray:\n",
    "        lat_deg=lat_deg.astype('float64')\n",
    "        \n",
    "    a=6378137 # equatorial radius\n",
    "    b=6356750 # polar radius\n",
    "    e=np.sqrt(1-(b/a)**2)\n",
    "    lat_rad=lat_deg*np.pi/180\n",
    "    earth_radius_m=a*np.sqrt(1-e**2*(np.sin(lat_rad))**2)\n",
    "    \n",
    "    return earth_radius_m\n",
    "\n",
    "##########################################################################################\n",
    "##########################################################################################\n",
    "    \n",
    "def compute_distance(lonA=0,latA=0,lonB=1,latB=0,verbose=False):\n",
    "    \n",
    "    # force float64 for input data to deal with default\n",
    "    # ndarray dtype which is float32\n",
    "    # and in this case, the computation is done in float32 \n",
    "    # which can lead to up to 8% relative\n",
    "    # error a distance of 1/12 deg (8/9 km).\n",
    "\n",
    "    lonA=np.array(lonA).astype('float64')\n",
    "    latA=np.array(latA).astype('float64')\n",
    "    lonB=np.array(lonB).astype('float64')    \n",
    "    latB=np.array(latB).astype('float64')\n",
    "    \n",
    "    \n",
    "    #then compute earth_radius median\n",
    "    #Earth_radius=6376*10**3 # in [m]\n",
    "    lat_med=0.5*(latA+latB)\n",
    "    Earth_radius=compute_earth_radius_elliptical(lat_med)\n",
    "    #print(lat_med,Earth_radius)\n",
    "    \n",
    "    # first, put them in radians\n",
    "    lonA_rad=lonA*np.pi/180\n",
    "    latA_rad=latA*np.pi/180\n",
    "    lonB_rad=lonB*np.pi/180\n",
    "    latB_rad=latB*np.pi/180\n",
    "    #print(lonA_rad,latA_rad,lonB_rad,latB_rad)\n",
    "    \n",
    "    if ((len(lonA.shape)!=0) & (len(lonB.shape) !=0)):\n",
    "        if (len(lonA) > len(lonB)): distance=np.zeros(lonA.shape)\n",
    "        else: distance=np.zeros(lonB.shape)\n",
    "    if ((len(lonA.shape)==0) & (len(lonB.shape) !=0)):\n",
    "        distance=np.zeros(lonB.shape)\n",
    "    if ((len(lonA.shape)!=0) & (len(lonB.shape) ==0)):\n",
    "        distance=np.zeros(lonA.shape)\n",
    "    if ((len(lonA.shape)==0) & (len(lonB.shape) ==0)): \n",
    "        distance=0.0\n",
    "    \n",
    "    eps=1e-13\n",
    "    is_A_an_array=False\n",
    "    is_B_an_array=False\n",
    "    try: \n",
    "        if np.size(lonA) > 1:is_A_an_array=True\n",
    "    except: \n",
    "        eps=eps\n",
    "    try: \n",
    "        if np.size(lonB) > 1:is_B_an_array=True\n",
    "    except: \n",
    "        eps=eps\n",
    "    \n",
    "    if verbose:\n",
    "        print(\"is_A_an_array,is_B_an_array:\")\n",
    "        print(is_A_an_array,is_B_an_array)\n",
    "    \n",
    "    \n",
    "    if ((is_A_an_array==True) & (is_B_an_array==True)):\n",
    "        #check where there is equality:\n",
    "        i_neq=np.where((abs(lonA-lonB)>eps) | (abs(latA-latB) > eps))\n",
    "\n",
    "        # then compute distance in [m]\n",
    "        distance[i_neq]=Earth_radius[i_neq]*np.arccos(np.sin(latA_rad[i_neq])*np.sin(latB_rad[i_neq]) + \\\n",
    "                                 np.cos(latA_rad[i_neq])*np.cos(latB_rad[i_neq])*np.cos(lonB_rad[i_neq]-lonA_rad[i_neq]))\n",
    "    \n",
    "        \n",
    "    if ( (is_A_an_array==False) & (is_B_an_array==True)):\n",
    "        #check where there is equality:\n",
    "        i_neq=np.where((abs(lonA-lonB)>eps) | (abs(latA-latB) > eps))\n",
    "        # then compute distance in [m]\n",
    "        AA=np.sin(latA_rad)*np.sin(latB_rad[i_neq])\n",
    "        BB=np.cos(latA_rad)*np.cos(latB_rad[i_neq])*np.cos(lonB_rad[i_neq]-lonA_rad)\n",
    "        cos_val=AA+BB\n",
    "        distance[i_neq]=Earth_radius[i_neq]*np.arccos(cos_val)\n",
    "    \n",
    "    \n",
    "    if ((is_A_an_array==True) & (is_B_an_array==False)):\n",
    "        #check where there is equality:\n",
    "        i_neq=np.where((abs(lonA-lonB)>eps) | (abs(latA-latB) > eps))\n",
    "\n",
    "        # then compute distance in [m]\n",
    "        distance[i_neq]=Earth_radius[i_neq]*np.arccos(np.sin(latA_rad[i_neq])*np.sin(latB_rad) + \\\n",
    "                                 np.cos(latA_rad[i_neq])*np.cos(latB_rad)*np.cos(lonB_rad-lonA_rad[i_neq]))\n",
    "    \n",
    "    \n",
    "    if ((is_A_an_array==False) & (is_B_an_array==False)):\n",
    "        if (abs(lonA-lonB)>eps) | (abs(latA-latB) > eps):\n",
    "            distance=Earth_radius*np.arccos(np.sin(latA_rad)*np.sin(latB_rad)+ \n",
    "                                 np.cos(latA_rad)*np.cos(latB_rad)*np.cos(lonB_rad-lonA_rad))\n",
    "    \n",
    "            \n",
    "    \n",
    "    \n",
    "    return distance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73d271fd-6605-4be5-8935-620cbe2c29ca",
   "metadata": {},
   "source": [
    "### II.f - In-situ observation grouping function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60a8929e-6784-4da3-a732-535b29e89e87",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_obs_groups(gp_crit,i_dataset,i_dataset_stf,df_in_situ,verbose=False):\n",
    "    \n",
    "    # create groups by spatio-temporal criterion (will be referred to as medium cube)\n",
    "\n",
    "    # transform capacity criterion into physical values\n",
    "    # degree are kept because the copernicus grids are regular in degrees. This is also the reason why below, \n",
    "    # distances computation are converted into equivalent degree at the equator.For the time resolution, it is a little bit trickier. \n",
    "    # the global attributes is a string 'P1D' ... \n",
    "    gp_max_x_deg=gp_crit['gp_max_x_n']*i_dataset_stf['reso_lon_deg']\n",
    "    gp_max_y_deg=gp_crit['gp_max_y_n']*i_dataset_stf['reso_lat_deg']\n",
    "    gp_max_t_ns=gp_crit['gp_max_t_n']*i_dataset_stf['reso_time_ns']\n",
    "    \n",
    "    \n",
    "    if verbose:\n",
    "        print(\"gp_max_x_deg = {0:.3f}, gp_max_y_deg = {1:.3f}, gp_max_t_ns = {2:d}\".format(gp_max_x_deg,gp_max_y_deg,gp_max_t_ns))\n",
    "    \n",
    "    # cast gp_max_t_ns in timedelta64 type (no more need to cast with new)\n",
    "    #gp_max_t_days_dt64=np.timedelta64(gp_max_t_days,'D')\n",
    "    \n",
    "    # first create a \"fictive\" observation id list:\n",
    "    list_obs_id = np.arange(0,df_in_situ.shape[0])\n",
    "    print(\"Initial number of observation:\", len(list_obs_id))\n",
    "    \n",
    "    i_group=0\n",
    "    \n",
    "    # create a dicionnary with the indexes of the various groups\n",
    "    group_of_obs={}\n",
    "\n",
    "    # First discard observations that are too old for the copernicus dataset:\n",
    "    print(\"Discarding observations that are too old\")\n",
    "    group_of_obs_too_old={}\n",
    "    i_too_old=np.where( (df_in_situ['DATE'] < i_dataset_stf['time_min']) )[0]\n",
    "    i_obs_group_n=list_obs_id[i_too_old]\n",
    "    group_of_obs_too_old=i_obs_group_n\n",
    "    list_obs_id=np.delete(list_obs_id,i_too_old)\n",
    "    print(\"Left number of observation:\", len(list_obs_id))\n",
    "\n",
    "    # Second discard observations that are too recent for the copernicus dataset:\n",
    "    print(\"Discarding observations that are too recent\")\n",
    "    group_of_obs_too_recent={}\n",
    "    i_too_recent=np.where( (df_in_situ['DATE'] > i_dataset_stf['time_max']) )[0]\n",
    "    i_obs_group_n=list_obs_id[i_too_recent]\n",
    "    group_of_obs_too_recent=i_obs_group_n\n",
    "    list_obs_id=np.delete(list_obs_id,i_too_recent)\n",
    "    print(\"Left number of observation:\", len(list_obs_id))\n",
    "    \n",
    "    while (len(list_obs_id) > 0) & (i_group<=df_in_situ.shape[0]) :\n",
    "    #while (len(list_obs_id) > 0) & (i_group<=3) :\n",
    "    \n",
    "        lon = df_in_situ['LON'][list_obs_id[0]]\n",
    "        lat = df_in_situ['LAT'][list_obs_id[0]]\n",
    "        dat = df_in_situ['DATE'][list_obs_id[0]]\n",
    "    \n",
    "        lon_obs_left=df_in_situ['LON'][list_obs_id]\n",
    "        lat_obs_left=df_in_situ['LAT'][list_obs_id]\n",
    "        dat_obs_left=df_in_situ['DATE'][list_obs_id]\n",
    "    \n",
    "        #compute equatorial equivalent distances\n",
    "        dist_m_2_deg_at_equat=(180/(np.pi*compute_earth_radius_elliptical(0)))\n",
    "        dist_lon_deg=compute_distance(lon,0,lon_obs_left,np.zeros(lon_obs_left.shape)) * dist_m_2_deg_at_equat\n",
    "        dist_lat_deg=compute_distance(0,lat,np.zeros(lat_obs_left.shape),lat_obs_left) * dist_m_2_deg_at_equat\n",
    "        dist_time_dt64=abs(dat-dat_obs_left)\n",
    "    \n",
    "        if verbose:\n",
    "            print(\"First observation position: {0:}, {1:.3f}째N {2:.3f}째E\".format(dat,lat,lon))\n",
    "            print(dist_lon_deg[:5])\n",
    "            print((lon-df_in_situ['LON'])[:5])\n",
    "            print(dist_lat_deg[:5])\n",
    "            print((lat-df_in_situ['LAT'])[:5])\n",
    "            print(dist_time_dt64[:5])\n",
    "\n",
    "        # Select close-by observations that are within the dataset time boundaries\n",
    "        # This time boundary filter can also be done before calling the function.\n",
    "        i_close_by=np.where((dist_lon_deg<=gp_max_x_deg) & \\\n",
    "                            (dist_lat_deg<=gp_max_y_deg) & \\\n",
    "                            (dist_time_dt64 <= gp_max_t_ns) )[0]\n",
    "        i_obs_group_n=list_obs_id[i_close_by]\n",
    "        group_of_obs[i_group]=i_obs_group_n\n",
    "        list_obs_id=np.delete(list_obs_id,i_close_by)\n",
    "        \n",
    "        if verbose:\n",
    "            print(\"i_close_by=\\n\", i_close_by)\n",
    "            print(\"i_obs_group_n=\\n\",i_obs_group_n)\n",
    "            print(\"list_obs_id after deleting\")\n",
    "            print(list_obs_id)\n",
    "            print(\"np.min(dat_obs_left[i_obs_group_n]),np.max(dat_obs_left[i_obs_group_n])\")\n",
    "            print(np.min(dat_obs_left[i_obs_group_n]),np.max(dat_obs_left[i_obs_group_n]))\n",
    "            print(\"np.min(dist_time_dt64[i_obs_group_n]),np.max(dist_time_dt64[i_obs_group_n])\")\n",
    "            print(np.min(dist_time_dt64[i_obs_group_n]),np.max(dist_time_dt64[i_obs_group_n]))\n",
    "            #print(\"[dist_time_dt64[i_obs_group_n] dat_obs_left[i_obs_group_n]]\")\n",
    "            #print([dist_time_dt64[i_obs_group_n] dat_obs_left[i_obs_group_n]])\n",
    "            print('\\n')\n",
    "            \n",
    "        print(\"i_group={0:d}, nb_elt_group={1:d},n_elt_left_to_group={2:d}\".format(i_group+1,len(i_close_by),len(list_obs_id)))\n",
    "            \n",
    "        i_group = i_group + 1  \n",
    "    if verbose: print(group_of_obs)\n",
    "    return group_of_obs,group_of_obs_too_old,group_of_obs_too_recent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a5d5e27-8495-4206-a089-d655d2341093",
   "metadata": {},
   "source": [
    "## III - Colocation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10a55b08-6836-4f9b-b417-6033e153fdfe",
   "metadata": {},
   "source": [
    "### III.a - configuration selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91231bbc-3029-44ef-af57-a731c8406b80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# choose the input depending on your needs (the output can be tuned\n",
    "access_type='ARGO_INDEX' # 'ARGO_DIRECT' or 'ARGO_CERBERE' or 'ARGO_INDEX' for the moment. this parameter will be used afterwards for plugging cerberized data\n",
    "cerbere_dir=\"C:/Users/ddobler/Documents/08_DD_scripts/09_FAIR-EASE/cerbere-data/\"\n",
    "\n",
    "#wmo='6901578' # long journey float\n",
    "wmo='6903024' # crosses 180 line (cycles 139 to 145 are on the West side of the line, the others on the East side)\n",
    "workflow_name='chl'\n",
    "\n",
    "clear_cache=True\n",
    "copernicus_method='subset' # 'lazy' or 'subset' : I kept both, can be tuned\n",
    "indexation_method='sel' # 'sel' or 'isel' or 'index' (in case of lazy access)\n",
    "record_format='NetCDF' # 'values' or 'NetCDF' or 'computation': either data are get (.values) or locally saved in a NetCDF file. Used for performance assessments\n",
    "verbose=False # the copernicus library can not yet be turned into quiet mode (but this works for informative prints)\n",
    "extract_data=True \n",
    "\n",
    "# Depending on your capacity, tune the grouping options\n",
    "gp_max_x_n=50#15#25#50#100 # i.e. within gp_max_x_n*reso_lon_deg, e.g. 200*0.04 = 8 deg\n",
    "gp_max_y_n=50#15#25#50#100\n",
    "gp_max_t_n=100#30#50#100#200\n",
    "\n",
    "\n",
    "\n",
    "if copernicus_method == 'subset':\n",
    "    record_format=\"NetCDF\"\n",
    "    indexation_method=\"\"\n",
    "\n",
    "gp_crit={}\n",
    "gp_crit['gp_max_x_n']=gp_max_x_n\n",
    "gp_crit['gp_max_y_n']=gp_max_y_n\n",
    "gp_crit['gp_max_t_n']=gp_max_t_n\n",
    "\n",
    "if verbose:\n",
    "    print(\"Estimate of the number of copernicus points to fetch: {:d}\".format( gp_max_x_n*gp_max_y_n*gp_max_t_n))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10f602cc-7ff9-4292-8e0e-36d2bcbafb4d",
   "metadata": {},
   "source": [
    "### III.b - IN-SITU data selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2372a077-0be8-4945-84ce-ae25cc09c705",
   "metadata": {},
   "outputs": [],
   "source": [
    "if access_type == 'ARGO_DIRECT':\n",
    "    df_in_situ,ds_in_situ=get_argo_data_from_direct_access(wmo,workflow_name)\n",
    "if access_type == 'ARGO_CERBERE':\n",
    "    df_in_situ,ds_in_situ=get_argo_data_from_cerbere_access(cerbere_dir,wmo,workflow_name)\n",
    "if access_type == 'ARGO_INDEX':\n",
    "    df_in_situ=get_argo_data_from_index(workflow_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43dc35f0-b92a-4b46-a96f-7c86e718f9de",
   "metadata": {},
   "source": [
    "### III.b - Define needed datasets and variables for Chlorophyll-A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d920e5e1-eb74-4538-917a-c7dd97aa1acf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve the copernicus dataset names and variables associated to the workflow\n",
    "l_dataset,d_dataset_var=get_workflow_dataset_and_var(workflow_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "222e2af7-0aa5-41ae-b676-313b5ba4d7fd",
   "metadata": {},
   "source": [
    "### III.c - spatial resolution and boundaries of the copernicus datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c166ce5-faf8-4b0e-b25a-768bfcb71a2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve the spatial resolution and boundaries of the copernicus datasets\n",
    "stime=time.time()\n",
    "#l_dataset_stf=get_resolution(workflow_name,method=copernicus_method,clear_cache=clear_cache,verbose=verbose)\n",
    "l_dataset_stf=get_resolution(workflow_name,method=copernicus_method,clear_cache=False,verbose=verbose)\n",
    "print('Execution time: {0:.1f} s'.format(time.time()-stime))\n",
    "# Performance from Ifremer site: 17 s using subset method vs 10s in lazy load."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fcc4909-c18a-4161-a3ae-e4513ff95aa1",
   "metadata": {},
   "source": [
    "### III.d - group extraction by geograpical criterion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5e7ab83-7369-43de-862a-36512f2b8d34",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e089c647-6483-466b-8ada-89bfd62f6def",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "stime=time.time()\n",
    "group_of_obs,group_of_obs_too_old,group_of_obs_too_recent=create_obs_groups(gp_crit,l_dataset[0],l_dataset_stf[l_dataset[0]],df_in_situ,verbose=False)\n",
    "print('Execution time: {0:.1f} s'.format(time.time()-stime))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3839e73-dfad-440a-a6cb-51876377bcc1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# this will be a function\n",
    "# get_copernicus_data(wmo,dataset_id,cycle_step,delta_x_px,delta_y_px,delta_t_days)\n",
    "# print lines will be commented.\n",
    "verbose=True\n",
    "delta_x_px=5\n",
    "delta_y_px=5\n",
    "delta_t_days=5\n",
    "\n",
    "log_file='perfo.log'\n",
    "file = open(log_file, 'a')\n",
    "line2write=\"date;location;group_crit;dataset_id;copernicus_method;record_format;cycle_step;\" +\\\n",
    "           \"execution_time[s];spatial_extension[square_degrees];temporal_extension[days];cache file size[B]\"\n",
    "print(line2write)\n",
    "file.write(line2write + '\\n')\n",
    "file.close()\n",
    "\n",
    "analysis_date=str(np.datetime64('today','D'))\n",
    "location=\"office\"\n",
    "\n",
    "\n",
    "for dataset_id in l_dataset:\n",
    "#for dataset_id in [dataset_chl]:\n",
    "\n",
    "    # for performance records:\n",
    "    n_obs_group=len(group_of_obs)\n",
    "    #sum_iobs=np.zeros(n_obs_group)\n",
    "    #sum_exet=np.zeros(n_obs_group)\n",
    "    #sum_Msp_ext=np.zeros(n_obs_group)\n",
    "    #sum_Mtp_ext=np.zeros(n_obs_group)\n",
    "    #sum_size_cache=np.zeros(n_obs_group)\n",
    "    \n",
    "\n",
    "    print(\"\\n\\n Workflow {0:s}; dataset {1:s} \".format(workflow_name,dataset_id))\n",
    "    print(\"Variables to extract: \",d_dataset_var[dataset_id])\n",
    "   \n",
    "    #compute associated delta_lon and delta_lat\n",
    "    i_dataset_stf=l_dataset_stf[dataset_id]\n",
    "    delta_lon=i_dataset_stf['reso_lon_deg']*delta_x_px\n",
    "    delta_lat=i_dataset_stf['reso_lat_deg']*delta_y_px\n",
    "\n",
    "    # These lines initiate or read the cache file\n",
    "    cache_dir=\"cache_files\"\n",
    "    if not os.path.exists(cache_dir):\n",
    "        os.mkdir(cache_dir)\n",
    "    cache_index_file = cache_dir + \"/cache_dowloaded_data_index.txt\"\n",
    "\n",
    "    if (os.path.exists(cache_index_file)) & (clear_cache):\n",
    "        os.remove(cache_index_file)\n",
    "        if verbose:print(\"the cache file was cleared\")\n",
    "            \n",
    "    \n",
    "    if not os.path.exists(cache_index_file):\n",
    "        file = open(cache_index_file, 'w')\n",
    "        line2write = \"dataset_id;date_min;date_max;lat_min;lat_max;lon_min;lon_max;cross_180;file_name\"\n",
    "        file.write(line2write + '\\n')\n",
    "        file.close()\n",
    "    \n",
    "    cache_index=pd.read_csv(cache_index_file,sep=\";\",dtype={'dataset_id': 'str', 'date_min': 'str', 'date_max': 'str',\n",
    "                                                            'lat_min' : 'float','lat_max':'float','lon_min' : 'float','lon_max':'float',\n",
    "                                                            'cross_180' : 'int','file_name':'str'})\n",
    "    \n",
    "    # If method is lazy, pre-load index for the dataset\n",
    "    if copernicus_method=='lazy':\n",
    "        ds_cop=copernicusmarine.open_dataset(dataset_id=dataset_id)\n",
    "        lat_cop=ds_cop['latitude']\n",
    "        lon_cop=ds_cop['longitude']\n",
    "        dat_cop=ds_cop['time']\n",
    "    \n",
    "    \n",
    "    for i_obs_group in range(n_obs_group):\n",
    "                \n",
    "        stime=time.time()\n",
    "        \n",
    "        spatial_extension_square_deg_max=0\n",
    "        temporal_extension_days_max=0\n",
    "        \n",
    "        print(\"\\n i_obs_group/n_obs_group = \",i_obs_group+1,\"/\",n_obs_group)\n",
    "            \n",
    "        outfile_dir=\"copernicus-data/worflow_{0:s}_xn_{1:03d}_yn_{2:03d}_tn_{3:03d}\".format(workflow_name,gp_crit['gp_max_x_n'],\n",
    "                                                                                            gp_crit['gp_max_x_n'],\n",
    "                                                                                            gp_crit['gp_max_t_n'])\n",
    "        if not os.path.exists(outfile_dir):\n",
    "            os.mkdir(outfile_dir)\n",
    "\n",
    "        # extract the grouped observations from df_in_situ:\n",
    "        idf=df_in_situ.iloc[group_of_obs[i_obs_group]]\n",
    "        dates_qc=np.array(idf['DATE_QC'])\n",
    "        position_qc=np.array(idf['POSITION_QC'])\n",
    "        latitudes=np.array(idf['LAT'])\n",
    "        longitudes=np.array(idf['LON'])\n",
    "        dates=np.array(idf['DATE'])\n",
    "        \n",
    "        # compute medium box boundaries, accounting for 180 crossing:\n",
    "        i_good_position=np.where(((position_qc==1) | (position_qc==2) | (position_qc==5) | (position_qc==8)) )\n",
    "        i_good_dates=np.where(((dates_qc==1) | (dates_qc==2) | (dates_qc==5) | (dates_qc==8)) )\n",
    "\n",
    "        # compute latitude boundaries\n",
    "        bbox_lat_min=max(i_dataset_stf['spat_lat_min'],np.min(latitudes[i_good_position])-delta_lat)\n",
    "        bbox_lat_max=min(i_dataset_stf['spat_lat_max'],np.max(latitudes[i_good_position])+delta_lat)\n",
    "        dlat=bbox_lat_max-bbox_lat_min\n",
    "\n",
    "        # compute longitude boundaries and check whether 180 was crossed\n",
    "        # 2024/12/18 update: let's do that much simplier\n",
    "        # Is the 180째 crossed (assumption: the medium cube will never reach 180째 large):\n",
    "        if np.max(longitudes[i_good_position])-np.min(longitudes[i_good_position]) < 180:\n",
    "            cross_180=0\n",
    "            bbox_lon_min=np.min(longitudes[i_good_position])-delta_lon\n",
    "            bbox_lon_max=np.max(longitudes[i_good_position])+delta_lon\n",
    "        else:\n",
    "            cross_180=1\n",
    "            bbox_lon_min=np.max(longitudes[i_good_position])-delta_lon\n",
    "            bbox_lon_max=np.min(longitudes[i_good_position])+delta_lon\n",
    "        dlon=bbox_lon_max-bbox_lon_min\n",
    "            \n",
    "        # compute time boundaries\n",
    "        bbox_dates_min=str(np.datetime_as_string(np.min(dates[i_good_dates])-np.timedelta64(delta_t_days,'D'),'s'))\n",
    "        bbox_dates_max=str(np.datetime_as_string(np.max(dates[i_good_dates])+np.timedelta64(delta_t_days,'D'),'s'))\n",
    "\n",
    "        # compute spatio-temporal extensions:\n",
    "        spatial_extenstion_square_deg=dlon*dlat\n",
    "        temporal_extension_days=np.timedelta64(np.max(dates[i_good_dates])-np.min(dates[i_good_dates]),'D') / np.timedelta64(1, 'D')\n",
    "        spatial_extenstion_square_deg_max=max(spatial_extenstion_square_deg,spatial_extension_square_deg_max)\n",
    "        temporal_extension_days_max=max(temporal_extension_days_max,temporal_extension_days)\n",
    "\n",
    "        # some debug printing:\n",
    "        if verbose:\n",
    "            print(\"obs_lat_min     = {0:.2f}\\t\\t, obs_lat_max     = {1:.2f}\".format(np.min(latitudes[i_good_position]),np.max(latitudes[i_good_position])))\n",
    "            print(\"bbox_lat_min    = {0:.2f}\\t\\t, bbox_lat_max    = {1:.2f}\\t dlat = {2:.2f}\".format(bbox_lat_min,bbox_lat_max,dlat))\n",
    "            print(\"obs_lon_min     = {0:.2f}\\t\\t, obs_lon_max     = {1:.2f}\".format(np.min(longitudes[i_good_position]),np.max(longitudes[i_good_position])))\n",
    "            print(\"bbox_lon_min    = {0:.2f}\\t\\t, bbox_lon_max    = {1:.2f}\\t dlon = {2:.2f}\".format(bbox_lon_min,bbox_lon_max,dlon))\n",
    "            print(\"bbox_dat_min = \",bbox_dates_min,\"\\t, bbox_dates_max = \",bbox_dates_max)\n",
    "            print(\"spatial_extension = {0:.2f} square_degrees temporal_extension = {1:.1f} days\".format(spatial_extenstion_square_deg,temporal_extension_days))\n",
    "    \n",
    "        \n",
    "        # Define cache file name\n",
    "        outfile_name=\"{0:s}_{1:s}_{2:s}_{3:.1f}_{4:.1f}_{5:.1f}_{6:.1f}.nc\".format(dataset_id,bbox_dates_min[:10],bbox_dates_max[:10],bbox_lat_min,\n",
    "                                                                            bbox_lat_max,bbox_lon_min,bbox_lon_max)\n",
    "        if verbose: print(\"outfile_name=\",outfile_name)\n",
    "\n",
    "        # Test whether the information are not yet downloaded:\n",
    "        index_line_exist=False\n",
    "        print(\"Number of lines in the cache index:\",cache_index['dataset_id'].size)\n",
    "        test_presence=np.array([])\n",
    "        if cache_index['dataset_id'].size > 0:\n",
    "            # dataset_id;date_min;date_max;lat_min;lat_max;lon_min;lon_max;cross_180;file_name\n",
    "            test_presence=np.where((cache_index['dataset_id'] == dataset_id) &\n",
    "                                   (cache_index['date_min']   <= bbox_dates_min) &\n",
    "                                   (cache_index['date_max']   >= bbox_dates_max) &\n",
    "                                   (cache_index['lat_min']    <= bbox_lat_min) &\n",
    "                                   (cache_index['lat_max']    >= bbox_lat_max) &\n",
    "                                   (cache_index['lon_min']    <= bbox_lon_min) &\n",
    "                                   (cache_index['lon_max']    >= bbox_lon_max) & \n",
    "                                   (cache_index['cross_180']  == cross_180))[0]\n",
    "            print(\"test_presence=\",test_presence)\n",
    "            if (test_presence.size > 0): \n",
    "                index_line_exist=True\n",
    "                print(\"the information to download already exists in a file, no need to download again from copernicus\")\n",
    "\n",
    "\n",
    "        if not index_line_exist:\n",
    "\n",
    "            if copernicus_method == 'lazy':\n",
    "                print(\"Subsetting data with the 'lazy' load\")\n",
    "                ii_dat=np.where((dat_cop > np.datetime64(bbox_dates_min)) & (dat_cop < np.datetime64(bbox_dates_max)))\n",
    "                ii_lat=np.where((lat_cop > bbox_lat_min) & (lat_cop < bbox_lat_max))\n",
    "                if cross_180 == 0:\n",
    "                    ii_lon=np.where((lon_cop > bbox_lon_min) & (lon_cop < bbox_lon_max))\n",
    "                if cross_180 == 1:\n",
    "                    ii_lon=np.where((lon_cop < bbox_lon_max) | (lon_cop > bbox_lon_min))\n",
    "\n",
    "                # Bench mark the different ways of subsetting an xarray dataset:\n",
    "                # the in-between solution:\n",
    "                if indexation_method == 'sel':\n",
    "                    ds_cop_group=ds_cop[d_dataset_var[dataset_id]].sel(time=dat_cop[ii_dat],\n",
    "                                                              latitude=lat_cop[ii_lat],\n",
    "                                                              longitude=lon_cop[ii_lon], \n",
    "                                                              method=\"nearest\")\n",
    "                #print(ii_dat[0])\n",
    "\n",
    "                # The longest (even if counter-intuitive)\n",
    "                if indexation_method == 'isel':\n",
    "                    ds_cop_group=ds_cop[d_dataset_var[dataset_id]].isel(time=ii_dat[0],\n",
    "                                                          latitude=ii_lat[0],\n",
    "                                                          longitude=ii_lon[0])\n",
    "                \n",
    "                # The fastest is direct indexing but can be done only by parameter, not the entire dataset.\n",
    "                if indexation_method == 'direct':\n",
    "                    ds_cop_group=ds_cop[d_dataset_var[dataset_id][0]][np.min(ii_dat[0]):np.max(ii_dat[0]),\n",
    "                                                                   np.min(ii_lat[0]):np.max(ii_lat[0]),\n",
    "                                                                   np.min(ii_lon[0]):np.max(ii_lon[0])]\n",
    "\n",
    "                \n",
    "                print(\"lazy indexing ended\")\n",
    "                if record_format == 'NetCDF': # 'values' or 'NetCDF'\n",
    "                    ds_cop_group.to_netcdf(outfile_dir+\"/\"+outfile_name)\n",
    "                    print(\"to_netcdf ended\")\n",
    "                if record_format == 'values': # 'values' or 'NetCDF'\n",
    "                    for iVAR in d_dataset_var[dataset_id]:\n",
    "                        print(\"Get variable \",iVAR)\n",
    "                        if indexation_method == 'direct':\n",
    "                            tmp=ds_cop_group.values\n",
    "                        else:\n",
    "                            tmp=ds_cop_group[iVAR].values                            \n",
    "                        print(tmp)\n",
    "                        print(\"to_values ended\")\n",
    "                if record_format == 'computation':\n",
    "                    print(\"entering \" + record_format + \" record format\")\n",
    "                    #print(ds_cop_group)\n",
    "                    ds_average=ds_cop_group.mean()\n",
    "                    print(ds_average)\n",
    "                    print(\"exiting \" + record_format + \" record format\")\n",
    "                    \n",
    "            \n",
    "\n",
    "            if copernicus_method=='subset':\n",
    "                print(\"Subsetting data with the subset method\")\n",
    "                if cross_180 == 1 :\n",
    "                    # There is a need to split the request in two:\n",
    "                    \n",
    "                    outfile_name_1=outfile_name + \"_1.nc\"\n",
    "                    xmin=i_dataset_stf['spat_lon_min']\n",
    "                    xmax=min(bbox_lon_max,i_dataset_stf['spat_lon_max'])\n",
    "                    if verbose:\n",
    "                        print(\"180 was crossed, splitting the subset request\")\n",
    "                        print(\"First request arguments:\")\n",
    "                        print(dataset_id,d_dataset_var[dataset_id],\n",
    "                             xmin,xmax,bbox_lat_min,bbox_lat_max,bbox_dates_min,bbox_dates_max,0,0,\n",
    "                             outfile_dir,outfile_name_1)\n",
    "                    get_cms_data(dataset_id,d_dataset_var[dataset_id],\n",
    "                         xmin,xmax,bbox_lat_min,bbox_lat_max,bbox_dates_min,bbox_dates_max,0,0,\n",
    "                         outfile_dir,outfile_name_1)\n",
    "                    \n",
    "                    outfile_name_2=outfile_name + \"_2.nc\"\n",
    "                    xmin=max(bbox_lon_min,i_dataset_stf['spat_lon_min'])\n",
    "                    xmax=i_dataset_stf['spat_lon_max']\n",
    "                    if verbose:\n",
    "                        print(\"second request arguments:\")\n",
    "                        print(dataset_id,d_dataset_var[dataset_id],\n",
    "                             xmin,xmax,bbox_lat_min,bbox_lat_max,bbox_dates_min,bbox_dates_max,0,0,\n",
    "                             outfile_dir,outfile_name_1)\n",
    "                    get_cms_data(dataset_id,d_dataset_var[dataset_id],\n",
    "                         xmin,xmax,bbox_lat_min,bbox_lat_max,bbox_dates_min,bbox_dates_max,0,0,\n",
    "                         outfile_dir,outfile_name_2)\n",
    "\n",
    "                    # merge results\n",
    "                    print(\"merging results into \" + outfile_dir + \"/\" + outfile_name)\n",
    "                    ds = xr.open_mfdataset([outfile_dir + \"/\" + outfile_name_1,outfile_dir + \"/\" + outfile_name_2], concat_dim=['longitude'], combine= \"nested\")\n",
    "                    ds.to_netcdf(outfile_dir + \"/\" + outfile_name)\n",
    "                    ds.close()\n",
    "                else:\n",
    "                    xmin=max(bbox_lon_min,i_dataset_stf['spat_lon_min'])\n",
    "                    xmax=min(bbox_lon_max,i_dataset_stf['spat_lon_max'])\n",
    "                    get_cms_data(dataset_id,d_dataset_var[dataset_id],\n",
    "                         xmin,xmax,bbox_lat_min,bbox_lat_max,bbox_dates_min,bbox_dates_max,0,0,\n",
    "                         outfile_dir,outfile_name)\n",
    "\n",
    "            # Saving in cache the \n",
    "            file = open(cache_index_file, 'a')\n",
    "            #\"dataset_id;date_min;date_max;lat_min;lat_max;lon_min;lon_max;cross_180;file_name\"\n",
    "            line2write = \"{0:s};{1:s};{2:s};{3:.6f};{4:.6f};{5:.6f};{6:.6f};{7:d};{8:s}\".format(dataset_id,bbox_dates_min,bbox_dates_max,\n",
    "                                                                                                bbox_lat_min,bbox_lat_max,bbox_lon_min,\n",
    "                                                                                                bbox_lon_max,cross_180,outfile_name)\n",
    "            file.write(line2write + '\\n')\n",
    "            file.close()\n",
    "\n",
    "        file = open(log_file, 'a')\n",
    "        if (test_presence.size == 0):\n",
    "            \n",
    "            if (copernicus_method == 'lazy') & (record_format == 'values') : \n",
    "                file_size=0\n",
    "            else:\n",
    "                file_size=os.path.getsize(outfile_dir+\"/\"+outfile_name)\n",
    "\n",
    "            if copernicus_method == 'lazy':\n",
    "                str_method=copernicus_method + \"_\" + indexation_method\n",
    "            else:\n",
    "                str_method=copernicus_method\n",
    "            \n",
    "            line2write_fmt=\"{0:s};{1:s};{2:d}_{3:d}_{4:d};{5:s};{6:s};{7:s};{8:.0f};{9:.5f};{10:.2f};{11:.0f};{12:.0f}\"\n",
    "            line2write=line2write_fmt.format(analysis_date,location,gp_crit['gp_max_x_n'],gp_crit['gp_max_y_n'],gp_crit['gp_max_t_n'],dataset_id,str_method,\n",
    "                                             record_format,i_obs_group,time.time()-stime,spatial_extenstion_square_deg_max,temporal_extension_days_max,\n",
    "                                             file_size)\n",
    "            \n",
    "            print(line2write)\n",
    "            file.write(line2write + '\\n')\n",
    "        file.close()\n",
    "\n",
    "\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8056632-8964-4274-adce-fb3258c72c18",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_in_situ_ard,ds_in_situ_ard=get_argo_data_from_direct_access(wmo,workflow_name)\n",
    "#df_in_situ_arc,ds_in_situ_arc=get_argo_data_from_cerbere_access(cerbere_dir,wmo,workflow_name)\n",
    "\n",
    "#print(np.min(df_in_situ_ard['LAT']),np.max(df_in_situ_ard['LAT']))\n",
    "#print(np.min(df_in_situ_arc['LAT']),np.max(df_in_situ_arc['LAT']))\n",
    "#print(np.min(df_in_situ_ard['LON']),np.max(df_in_situ_ard['LON']))\n",
    "#print(np.min(df_in_situ_arc['LON']),np.max(df_in_situ_arc['LON']))\n",
    "#print(np.min(df_in_situ_ard['CYCLE']),np.max(df_in_situ_ard['CYCLE']))\n",
    "#print(np.min(df_in_situ_arc['CYCLE']),np.max(df_in_situ_arc['CYCLE']))\n",
    "#xc=df_in_situ_arc['LON']\n",
    "#xc[np.where(xc<0)[0]]=xc[np.where(xc<0)[0]]+360\n",
    "\n",
    "#xd=df_in_situ_ard['LON']\n",
    "#xd[np.where(xd<0)[0]]=xd[np.where(xd<0)[0]]+360\n",
    "\n",
    "#plt.figure()\n",
    "#plt.plot(xc,df_in_situ_arc['LAT'],'.',markersize=10)\n",
    "#plt.plot(xd,df_in_situ_ard['LAT'],'.',markersize=5)\n",
    "\n",
    "#plt.figure()\n",
    "#plt.plot(df_in_situ_arc['CYCLE'],df_in_situ_arc['LAT'],'.',markersize=10)\n",
    "#plt.plot(df_in_situ_ard['CYCLE'],df_in_situ_ard['LAT'],'*',markersize=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d709f22-3a14-4b9c-a526-32bdffdb7121",
   "metadata": {},
   "outputs": [],
   "source": [
    "cache_index=pd.read_csv(cache_index_file,sep=\";\",dtype={'wmo': 'str', 'dataset_id': 'str', 'cycle_step': 'int',\n",
    "                                                        'cycle_min' : 'int','cycle_max':'int',\n",
    "                                                        'delta_x_px':'int','delta_y_px':'int','delta_t_days':'int',\n",
    "                                                        'file_name':'str'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "552118fd-f36d-4046-987f-4fe435c1c120",
   "metadata": {},
   "outputs": [],
   "source": [
    "cycle_to_plot=136\n",
    "ds=Dataset(SPROF_FILE,'r')\n",
    "ds.set_auto_mask(False) # to avoid masked array, a little bit more tricky to manage\n",
    "CHL=ds.variables[\"CHLA\"][:]\n",
    "PRES=ds.variables[\"PRES\"][:]\n",
    "ds.close()\n",
    "\n",
    "# select the data to plot using several criteria\n",
    "\n",
    "# choose the cycle\n",
    "i_cycle_to_plot=np.where(cycles==cycle_to_plot)[:]\n",
    "CHL_icycle=CHL[i_cycle_to_plot]\n",
    "PRES_icycle=PRES[i_cycle_to_plot]\n",
    "lon_icycle=longitudes[i_cycle_to_plot]\n",
    "lat_icycle=latitudes[i_cycle_to_plot]\n",
    "tim_icycle=dates[i_cycle_to_plot]\n",
    "\n",
    "#choose valid data\n",
    "i_ok=np.where(CHL_icycle < 99999)\n",
    "CHL_ic_ok=CHL_icycle[i_ok]\n",
    "PRES_ic_ok=PRES_icycle[i_ok]\n",
    "\n",
    "#choose surface data\n",
    "i_surface=np.where(PRES_ic_ok==np.min(PRES_ic_ok))\n",
    "CHL_to_plot=CHL_ic_ok[i_surface]\n",
    "\n",
    "print(CHL_to_plot)\n",
    "copernicus_file={}\n",
    "# select file\n",
    "for dataset_id in [dataset_chl]:\n",
    "    i_copernicus_file=np.where((cache_index['wmo']       == wmo) & \n",
    "                        (cache_index['dataset_id']   == dataset_id) &\n",
    "                        (cache_index['cycle_min']    <= cycle_to_plot) &\n",
    "                        (cache_index['cycle_max']    >= cycle_to_plot) &\n",
    "                        (cache_index['delta_x_px']   == delta_x_px) &\n",
    "                        (cache_index['delta_y_px']   == delta_y_px) &\n",
    "                        (cache_index['delta_t_days'] == delta_t_days))[0]\n",
    "    print(i_copernicus_file)\n",
    "    if (i_copernicus_file.size > 0):\n",
    "        copernicus_file[dataset_id]=cache_index['file_name'][i_copernicus_file[0]]\n",
    "        if os.path.exists(copernicus_file[dataset_id]): \n",
    "            print(\"Ok, the file has been found and exists: \\n\",copernicus_file[dataset_id])\n",
    "        else:\n",
    "            print(\"Copernicus file is not here: download step to do\")\n",
    "\n",
    "    ds=Dataset(copernicus_file[dataset_id],'r')\n",
    "    ds.set_auto_mask(False)\n",
    "    lat_c=ds.variables['latitude'][:]\n",
    "    lon_c=ds.variables['longitude'][:]\n",
    "    tim_c=np.array(nc.num2date(ds.variables['time'][:], ds.variables['time'].units, only_use_cftime_datetimes=False)).astype('datetime64[ms]')\n",
    "    print(tim_c)\n",
    "\n",
    "    print(lon_icycle,lon_c)\n",
    "    dist_lon=compute_distance(lon_icycle,0,lon_c,np.zeros(lon_c.shape))\n",
    "    dist_lat=compute_distance(0,lat_icycle,np.zeros(lat_c.shape),lat_c)\n",
    "    dist_time=abs(tim_c-tim_icycle)\n",
    "\n",
    "    dist_lon_thresh=compute_distance(delta_lon,0,0,0)\n",
    "    dist_lat_thresh=compute_distance(0,delta_lat,0,0)\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "# cycle BBOX\n",
    "\n",
    "# display result"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
